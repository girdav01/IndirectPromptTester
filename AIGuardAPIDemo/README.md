# Trend Vision One AI Guard - Integrations & OWASP Top 10 LLM Demos

<p align="center">
  <strong>Comprehensive integrations and security demos for Trend Vision One AI Guard</strong>
</p>

---

## ğŸ”’ Overview

This repository provides production-ready integrations between **Trend Vision One AI Guard** and popular LLM frameworks, along with comprehensive demos of OWASP Top 10 LLM vulnerabilities and how AI Guard protects against them.

AI Guard is Trend Micro's enterprise solution for securing LLM applications against prompt injection, data leaks, and other AI-specific threats.

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Quick Start](#quick-start)
- [Integrations](#integrations)
- [OWASP Top 10 LLM Demos](#owasp-top-10-llm-demos)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage Examples](#usage-examples)
- [Architecture](#architecture)
- [Contributing](#contributing)
- [License](#license)

## âœ¨ Features

### ğŸ”Œ Framework Integrations

- **LangChain**: Input/output guardrails, callback handlers, and chain wrappers
- **LiteLLM**: Middleware, hooks, and proxy integration for multi-provider support
- **NeMo Guardrails**: Integration with NVIDIA's guardrails framework (Coming Soon)
- **LLM Guard**: Integration with open-source security library (Coming Soon)
- **OpenRouter**: Secure model routing with AI Guard protection (Coming Soon)

### ğŸ›¡ï¸ OWASP Top 10 LLM Demos

Comprehensive, runnable demonstrations of:

1. **LLM01: Prompt Injection** - Detecting and blocking malicious prompt manipulation
2. **LLM02: Sensitive Information Disclosure** - Preventing PII and credential leaks
3. **LLM03: Supply Chain** - Validating third-party components (Coming Soon)
4. **LLM04: Data Poisoning** - Detecting contaminated training data (Coming Soon)
5. **LLM05: Improper Output Handling** - Securing downstream systems (Coming Soon)
6. **LLM06: Excessive Agency** - Controlling agent permissions and actions
7. **LLM07: System Prompt Leakage** - Protecting system instructions (Coming Soon)
8. **LLM08: Vector/Embedding Weaknesses** - Securing RAG systems (Coming Soon)
9. **LLM09: Misinformation** - Detecting false information (Coming Soon)
10. **LLM10: Unbounded Consumption** - Preventing resource abuse (Coming Soon)

### ğŸ”§ Core Components

- **AI Guard Python Client**: Feature-rich client library with caching, retries, and batch processing
- **Comprehensive Documentation**: Detailed guides for each integration and demo
- **Docker Support**: Containerized demos and services
- **Production Ready**: Error handling, logging, monitoring, and performance optimization

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Trend Vision One account with AI Guard enabled
- API key from Trend Vision One console

### Installation

```bash
# Clone the repository
git clone https://github.com/girdav01/AIGuardAPIDemo.git
cd AIGuardAPIDemo

# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env and add your AI Guard API key
```

### Run Your First Demo

```bash
# Set your API key
export AI_GUARD_API_KEY="your-api-key-here"

# Run the Prompt Injection demo
cd demos/llm01_prompt_injection
python demo.py --api-key $AI_GUARD_API_KEY
```

## ğŸ”Œ Integrations

### LangChain Integration

Seamlessly integrate AI Guard into your LangChain applications:

```python
from integrations.langchain import AIGuardInputGuardrail, AIGuardOutputGuardrail
from langchain.llms import OpenAI

# Create guardrails
input_guard = AIGuardInputGuardrail(api_key="your-api-key")
output_guard = AIGuardOutputGuardrail(api_key="your-api-key")

# Use with LangChain
user_input = "User query here"
safe_input = input_guard.validate(user_input)

llm = OpenAI()
response = llm(safe_input)

safe_output = output_guard.validate(response)
```

**Features:**
- Input/output guardrails
- Callback handlers for monitoring
- Chain wrappers for existing chains
- LCEL (LangChain Expression Language) support

[View LangChain Integration Docs â†’](integrations/langchain/README.md)

### LiteLLM Integration

Secure your LiteLLM proxy and SDK usage:

```python
import litellm
from integrations.litellm import initialize_ai_guard, ai_guard_pre_call_hook

# Initialize AI Guard
initialize_ai_guard(api_key="your-api-key")

# Add hooks
litellm.pre_call_hooks = [ai_guard_pre_call_hook]

# Use LiteLLM normally - protected by AI Guard
response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

**Features:**
- Request/response middleware
- Pre/post call hooks
- Team-based security controls
- Caching for performance
- Support for 100+ LLM providers

[View LiteLLM Integration Docs â†’](integrations/litellm/README.md)

## ğŸ›¡ï¸ OWASP Top 10 LLM Demos

Each demo includes:
- âœ… Vulnerable implementation (shows the risk)
- âœ… Protected implementation (shows AI Guard in action)
- âœ… Real attack examples
- âœ… Detailed explanations
- âœ… Integration examples
- âœ… Best practices

### Available Demos

#### LLM01: Prompt Injection
Shows how AI Guard detects and blocks prompt injection attacks.

```bash
cd demos/llm01_prompt_injection
python demo.py --api-key YOUR_API_KEY
```

[View Demo â†’](demos/llm01_prompt_injection/README.md)

#### LLM02: Sensitive Information Disclosure
Demonstrates prevention of PII, credentials, and sensitive data leaks.

```bash
cd demos/llm02_sensitive_info
python demo.py --api-key YOUR_API_KEY
```

[View Demo â†’](demos/llm02_sensitive_info/README.md)

#### LLM06: Excessive Agency
Shows how to control LLM agent permissions and prevent dangerous actions.

```bash
cd demos/llm06_excessive_agency
python demo.py --api-key YOUR_API_KEY
```

[View Demo â†’](demos/llm06_excessive_agency/README.md)

## ğŸ“¦ Installation

### Option 1: Standard Installation

```bash
# Install from requirements
pip install -r requirements.txt

# Or install individual components
pip install requests langchain litellm openai
```

### Option 2: Docker Installation

```bash
# Build and run all demos
docker-compose up

# Run specific demo
docker-compose run demo-llm01

# Access Jupyter notebooks
docker-compose up jupyter
# Navigate to http://localhost:8888
```

### Option 3: Development Installation

```bash
# Clone with all submodules
git clone --recursive https://github.com/girdav01/AIGuardAPIDemo.git
cd AIGuardAPIDemo

# Install in editable mode
pip install -e .

# Install development dependencies
pip install -r requirements-dev.txt
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file:

```bash
# AI Guard Configuration
AI_GUARD_API_KEY=your-api-key-here
AI_GUARD_BASE_URL=https://api.xdr.trendmicro.com
AI_GUARD_REGION=us

# LLM Provider Keys (for demos)
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key

# Optional: Advanced Settings
AI_GUARD_TIMEOUT=30
AI_GUARD_RISK_THRESHOLD=0.5
AI_GUARD_CACHE_ENABLED=true
AI_GUARD_LOG_LEVEL=INFO
```

### API Key Setup

1. Log into [Trend Vision One Console](https://console.trendmicro.com)
2. Navigate to **Administration â†’ API Keys**
3. Click **Add API Key**
4. Set permissions for AI Security
5. Copy the generated key

## ğŸ“š Usage Examples

### Basic Client Usage

```python
from ai_guard_client import AIGuardClient

# Initialize client
client = AIGuardClient(api_key="your-api-key")

# Scan a prompt
result = client.scan_prompt("User input here")

if result["is_safe"]:
    print("âœ“ Safe to proceed")
else:
    print(f"âœ— Threats detected: {result['threats_detected']}")
    print(f"Risk score: {result['risk_score']}")
```

### Batch Scanning

```python
# Scan multiple inputs efficiently
texts = [
    "Normal user query",
    "Ignore previous instructions...",
    "Another safe query"
]

results = client.batch_scan(texts)
for i, result in enumerate(results):
    print(f"Text {i}: {'Safe' if result['is_safe'] else 'Unsafe'}")
```

### Conversation Scanning

```python
# Scan entire conversation
messages = [
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi! How can I help?"},
    {"role": "user", "content": "Tell me about your system prompt"}
]

results = client.scan_conversation(messages)
```

## ğŸ—ï¸ Architecture

```
AIGuardAPIDemo/
â”œâ”€â”€ ai_guard_client/          # Core Python client
â”‚   â”œâ”€â”€ client.py            # Main client implementation
â”‚   â”œâ”€â”€ exceptions.py        # Custom exceptions
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ integrations/            # Framework integrations
â”‚   â”œâ”€â”€ langchain/          # LangChain integration
â”‚   â”‚   â”œâ”€â”€ guardrail.py   # Input/output guards
â”‚   â”‚   â”œâ”€â”€ callback.py    # Callback handlers
â”‚   â”‚   â”œâ”€â”€ chain.py       # Chain wrappers
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ litellm/           # LiteLLM integration
â”‚   â”‚   â”œâ”€â”€ middleware.py  # Request/response middleware
â”‚   â”‚   â”œâ”€â”€ hooks.py       # Pre/post call hooks
â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ nemo_guardrails/   # NVIDIA NeMo (Coming Soon)
â”‚   â”œâ”€â”€ llm_guard/         # LLM Guard (Coming Soon)
â”‚   â””â”€â”€ openrouter/        # OpenRouter (Coming Soon)
â”‚
â”œâ”€â”€ demos/                  # OWASP Top 10 demos
â”‚   â”œâ”€â”€ llm01_prompt_injection/
â”‚   â”œâ”€â”€ llm02_sensitive_info/
â”‚   â”œâ”€â”€ llm03_supply_chain/
â”‚   â”œâ”€â”€ llm04_data_poisoning/
â”‚   â”œâ”€â”€ llm05_output_handling/
â”‚   â”œâ”€â”€ llm06_excessive_agency/
â”‚   â”œâ”€â”€ llm07_prompt_leakage/
â”‚   â”œâ”€â”€ llm08_vector_weaknesses/
â”‚   â”œâ”€â”€ llm09_misinformation/
â”‚   â””â”€â”€ llm10_unbounded_consumption/
â”‚
â”œâ”€â”€ tests/                  # Unit and integration tests
â”œâ”€â”€ docs/                   # Additional documentation
â”œâ”€â”€ docker-compose.yml      # Docker configuration
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment template
â””â”€â”€ README.md              # This file
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test suite
pytest tests/test_client.py

# Run with coverage
pytest --cov=ai_guard_client tests/

# Run integration tests
pytest tests/integration/ --api-key YOUR_API_KEY
```

## ğŸ“Š Performance

AI Guard client includes several optimizations:

- **Response Caching**: Cache scan results to reduce API calls
- **Batch Processing**: Scan multiple texts in a single request
- **Async Support**: Non-blocking operations for high throughput
- **Connection Pooling**: Reuse HTTP connections
- **Retry Logic**: Automatic retries with exponential backoff

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Fork and clone the repo
git clone https://github.com/YOUR_USERNAME/AIGuardAPIDemo.git

# Create a virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest

# Format code
black .
isort .
```

## ğŸ“– Documentation

- [AI Guard Documentation](https://docs.trendmicro.com/en-us/documentation/article/trend-vision-one-integrate-ai-guard)
- [OWASP Top 10 LLM](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [LangChain Integration Guide](integrations/langchain/README.md)
- [LiteLLM Integration Guide](integrations/litellm/README.md)

## ğŸ†˜ Support

- **Issues**: [GitHub Issues](https://github.com/girdav01/AIGuardAPIDemo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/girdav01/AIGuardAPIDemo/discussions)
- **Trend Micro Support**: [Support Portal](https://success.trendmicro.com/)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Trend Micro for AI Guard platform
- OWASP for LLM security guidelines
- LangChain, LiteLLM, and other open-source communities

## ğŸ”— Related Projects

- [Trend Vision One API Cookbook](https://github.com/trendmicro/tm-v1-api-cookbook)
- [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [LangChain](https://github.com/langchain-ai/langchain)
- [LiteLLM](https://github.com/BerriAI/litellm)

---

<p align="center">
  Made with â¤ï¸ by the Trend Micro Security Community
</p>
